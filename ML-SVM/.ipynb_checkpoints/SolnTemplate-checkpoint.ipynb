{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CMPE 462 - Project 2<br>Implementing an SVM Classifier<br>Due: May 18, 2020, 23:59</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Student ID1:** 2015400162\n",
    "* **Student ID2:** 2015400159\n",
    "* **Student ID3:** 2015400090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this project, you are going to implement SVM. For this purpose, a data set (data.mat) is given to you. You can load the mat dataset into Python using the function `loadmat` in `Scipy.io`. When you load the data, you will obtain a dictionary object, where `X` stores the data matrix and `Y` stores the labels. You can use the first 150 samples for training and the rest for testing. In this project, you will use the software package [`LIBSVM`](http://www.csie.ntu.edu.tw/~cjlin/libsvm/) to implement SVM. Note that `LIBSVM` has a [`Python interface`](https://github.com/cjlin1/libsvm/tree/master/python), so you can call the SVM functions in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import scipy\n",
    "from libsvm.svmutil import *\n",
    "import numpy as np\n",
    "\n",
    "data = scio.loadmat(\"data.mat\")\n",
    "x,y=data[\"X\"],np.reshape(data[\"Y\"],(270))\n",
    "X_train = x[:150]\n",
    "Y_train = y[:150]\n",
    "X_test = x[150:]\n",
    "Y_test = y[150:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - 30 pts\n",
    "\n",
    "Train a hard margin linear SVM and report both train and test classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 99.3333% (149/150) (classification)\n",
      "Accuracy = 78.3333% (94/120) (classification)\n"
     ]
    }
   ],
   "source": [
    "model = svm_train(Y_train,X_train, \"-s 0 -c 100\")\n",
    "\n",
    "p_label, p_acc, p_val = svm_predict(Y_train, X_train, model)\n",
    "p_label, p_acc, p_val = svm_predict(Y_test, X_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - 40 pts\n",
    "\n",
    "Train soft margin SVM for different values of the parameter $C$, and with different kernel functions. Systematically report your results. For instance, report the performances of different kernels for a fixed $C$, then report the performance for different $C$ values for a fixed kernel, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data results for C = 50, linear kernel function:\n",
      "Accuracy = 88.6667% (133/150) (classification)\n",
      "Test data results for C = 50, linear kernel function:\n",
      "Accuracy = 81.6667% (98/120) (classification)\n",
      "\n",
      "Training data results for C = 50, polynomial kernel function:\n",
      "Accuracy = 97.3333% (146/150) (classification)\n",
      "Test data results for C = 50, polynomial kernel function:\n",
      "Accuracy = 80% (96/120) (classification)\n",
      "\n",
      "Training data results for C = 50, radial basis kernel function:\n",
      "Accuracy = 98% (147/150) (classification)\n",
      "Test data results for C = 50, radial basis kernel function:\n",
      "Accuracy = 77.5% (93/120) (classification)\n",
      "\n",
      "Training data results for C = 50, sigmoid kernel function:\n",
      "Accuracy = 77.3333% (116/150) (classification)\n",
      "Test data results for C = 50, sigmoid kernel function:\n",
      "Accuracy = 72.5% (87/120) (classification)\n",
      "\n",
      "---\n",
      "Let us choose the linear kernel function, as it has the highest test validation accuracy.\n",
      "\n",
      "Training data results for C = 1, linear kernel function:\n",
      "Accuracy = 86.6667% (130/150) (classification)\n",
      "Test data results for C = 1, linear kernel function:\n",
      "Accuracy = 85% (102/120) (classification)\n",
      "\n",
      "Training data results for C = 2, linear kernel function:\n",
      "Accuracy = 88% (132/150) (classification)\n",
      "Test data results for C = 2, linear kernel function:\n",
      "Accuracy = 83.3333% (100/120) (classification)\n",
      "\n",
      "Training data results for C = 5, linear kernel function:\n",
      "Accuracy = 88.6667% (133/150) (classification)\n",
      "Test data results for C = 5, linear kernel function:\n",
      "Accuracy = 81.6667% (98/120) (classification)\n",
      "\n",
      "Training data results for C = 10, linear kernel function:\n",
      "Accuracy = 88.6667% (133/150) (classification)\n",
      "Test data results for C = 10, linear kernel function:\n",
      "Accuracy = 81.6667% (98/120) (classification)\n",
      "\n",
      "Training data results for C = 100, linear kernel function:\n",
      "Accuracy = 88.6667% (133/150) (classification)\n",
      "Test data results for C = 100, linear kernel function:\n",
      "Accuracy = 81.6667% (98/120) (classification)\n"
     ]
    }
   ],
   "source": [
    "# Let us test the performances of different kernels for C = 50\n",
    "\n",
    "model = svm_train(Y_train,X_train, \"-s 0 -c 50 -t 0\") # 0 -- linear: u'*v\n",
    "\n",
    "print(\"Training data results for C = 50, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_train, X_train, model)\n",
    "print(\"Test data results for C = 50, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_test, X_test, model)\n",
    "\n",
    "\n",
    "model = svm_train(Y_train,X_train, \"-s 0 -c 50 -t 1\") # 1 -- polynomial: (gamma*u'*v + coef0)^degree\n",
    "\n",
    "print(\"\\nTraining data results for C = 50, polynomial kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_train, X_train, model)\n",
    "print(\"Test data results for C = 50, polynomial kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_test, X_test, model)\n",
    "\n",
    "\n",
    "model = svm_train(Y_train,X_train, \"-s 0 -c 50 -t 2\") # 2 -- radial basis function: exp(-gamma*|u-v|^2)\n",
    "\n",
    "print(\"\\nTraining data results for C = 50, radial basis kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_train, X_train, model)\n",
    "print(\"Test data results for C = 50, radial basis kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_test, X_test, model)\n",
    "\n",
    "\n",
    "model = svm_train(Y_train,X_train, \"-s 0 -c 50 -t 3\") # 3 -- sigmoid: tanh(gamma*u'*v + coef0)\n",
    "\n",
    "print(\"\\nTraining data results for C = 50, sigmoid kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_train, X_train, model)\n",
    "print(\"Test data results for C = 50, sigmoid kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_test, X_test, model)\n",
    "\n",
    "\n",
    "print(\"\\n---\\nLet us choose the linear kernel function, as it has the highest test validation accuracy.\")\n",
    "\n",
    "model_1 = svm_train(Y_train,X_train, \"-s 0 -c 1 -t 0\")\n",
    "\n",
    "print(\"\\nTraining data results for C = 1, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_train, X_train, model_1)\n",
    "print(\"Test data results for C = 1, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_test, X_test, model_1)\n",
    "\n",
    "\n",
    "model_2 = svm_train(Y_train,X_train, \"-s 0 -c 2 -t 0\")\n",
    "\n",
    "print(\"\\nTraining data results for C = 2, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_train, X_train, model_2)\n",
    "print(\"Test data results for C = 2, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_test, X_test, model_2)\n",
    "\n",
    "\n",
    "model_5 = svm_train(Y_train,X_train, \"-s 0 -c 5 -t 0\")\n",
    "\n",
    "print(\"\\nTraining data results for C = 5, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_train, X_train, model_5)\n",
    "print(\"Test data results for C = 5, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_test, X_test, model_5)\n",
    "\n",
    "\n",
    "model_10 = svm_train(Y_train,X_train, \"-s 0 -c 10 -t 0\")\n",
    "\n",
    "print(\"\\nTraining data results for C = 10, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_train, X_train, model_10)\n",
    "print(\"Test data results for C = 10, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_test, X_test, model_10)\n",
    "\n",
    "\n",
    "model_100 = svm_train(Y_train,X_train, \"-s 0 -c 100 -t 0\")\n",
    "\n",
    "print(\"\\nTraining data results for C = 100, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_train, X_train, model_100)\n",
    "print(\"Test data results for C = 100, linear kernel function:\")\n",
    "p_label, p_acc, p_val = svm_predict(Y_test, X_test, model_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - 15 pts\n",
    "\n",
    "Please report how the number of support vectors changes as the value of $C$ increases (while all other parameters remain the same). Discuss whether your observations match the theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As you can see below, number of support vectors decreases as the value of ùê∂ increases.\n",
      "58\n",
      "56\n",
      "54\n",
      "51\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print (\"As you can see below, number of support vectors decreases as the value of ùê∂ increases.\")\n",
    "# When C is large, it means we care more about violating the margin, which gets us closer to the hard-margin SVM.\n",
    "#If we allow bigger margin, we should have a large number of support vectors. \n",
    "#So,getting closer to hard margin via increasing C, gives us less number of support vectors as you see.\n",
    "\n",
    "print (model_1.get_nr_sv())\n",
    "print (model_2.get_nr_sv())\n",
    "print (model_5.get_nr_sv())\n",
    "print (model_10.get_nr_sv())\n",
    "print (model_100.get_nr_sv())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - 15 pts\n",
    "\n",
    "Please investigate the changes in the hyperplane when you remove one of the support vectors, vs., one data point that is not a support vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def weightFinder(model):\n",
    "    SVs=model.get_SV()\n",
    "    coef=model.get_sv_coef()\n",
    "    weight=[0 for i in range(X_train.shape[1])]\n",
    "    for i in range(len(coef)):\n",
    "        for j in range(1,X_train.shape[1]+1):\n",
    "            if not (SVs[i].get(j) is None):\n",
    "                weight[j-1]+=SVs[i].get(j)*coef[i][0]\n",
    "    return weight,model.get_sv_indices()\n",
    "def deletePoints(model):\n",
    "    SV_ind=model.get_sv_indices()[-1] #get index of the last support vector\n",
    "    data_ind=149  #some index that is not a support vector\n",
    "    X_train_1,Y_train_1= np.concatenate((X_train[:SV_ind], X_train[SV_ind+1:]), axis=0),np.concatenate((Y_train[:SV_ind], Y_train[SV_ind+1:]), axis=0) #\n",
    "    X_train_2,Y_train_2=np.concatenate((X_train[:data_ind], X_train[data_ind+1:]), axis=0),np.concatenate((Y_train[:data_ind], Y_train[data_ind+1:]), axis=0)\n",
    "    return X_train_1,Y_train_1,X_train_2,Y_train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before removal:\n",
      "[-1.308960694997034, 0.5255770615861053, 1.3470379341678012, 1.3786551708896582, 1.6279048137817682, 0.1613750898546158, 0.13202185984296477, -1.8521656608970147, -0.02557950197261505, -0.07641398739398397, 0.06905809226138615, 2.724943229661587, 0.44802622958216887]\n",
      "SV indices before removal: \n",
      "[3, 14, 18, 29, 31, 38, 41, 45, 48, 59, 62, 68, 70, 92, 98, 102, 106, 109, 111, 113, 117, 131, 132, 145, 147, 2, 4, 6, 11, 12, 15, 23, 24, 27, 32, 53, 61, 69, 75, 77, 85, 86, 88, 97, 125, 126, 133, 135, 140, 144]\n",
      "\n",
      "Weights after removal of a SV data point:\n",
      "[-1.1673892919251294, 0.49414569555652577, 1.3248126614021913, 1.2582097233898324, 1.3529999359566318, 0.02632799958355747, 0.04065292518397712, -1.9722469759555192, -0.013711859744105936, -0.08426744210563442, 0.28936196029015093, 2.8689328636374505, 0.3817702423921645]\n",
      "SV indices after removal of a SV data point: \n",
      "[3, 14, 18, 29, 31, 38, 41, 45, 48, 59, 62, 68, 70, 92, 98, 102, 106, 109, 111, 113, 117, 131, 132, 146, 147, 2, 4, 6, 11, 12, 15, 23, 24, 27, 32, 53, 61, 69, 75, 77, 85, 86, 88, 97, 125, 126, 133, 135, 140, 144]\n",
      "Set difference between SV's indices before and after removal of a SV data point.\n",
      "[146]\n",
      "\n",
      "Weights after removal of a non-SV data point:\n",
      "[-1.3086316401552454, 0.5256006629586949, 1.3469603720069614, 1.3786885050173652, 1.6271960978388833, 0.16120481773926087, 0.1318102270245589, -1.8519201307924433, -0.02569283304457315, -0.07611806373145669, 0.06924638672929007, 2.72501607171343, 0.4479868715526578]\n",
      "SV indices after removal of a non-SV data point: \n",
      "[3, 14, 18, 29, 31, 38, 41, 45, 48, 59, 62, 68, 70, 92, 98, 102, 106, 109, 111, 113, 117, 131, 132, 145, 147, 2, 4, 6, 11, 12, 15, 23, 24, 27, 32, 53, 61, 69, 75, 77, 85, 86, 88, 97, 125, 126, 133, 135, 140, 144]\n",
      "Set difference between SV's indices before and after removal of a non-SV data point.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "weights,sv_ind=weightFinder(model_100)\n",
    "print(\"Weights before removal:\")\n",
    "print(weights)\n",
    "print(\"SV indices before removal: \")\n",
    "print(sv_ind)\n",
    "\n",
    "#removal of points\n",
    "X_train_1,Y_train_1,X_train_2,Y_train_2=deletePoints(model_100)\n",
    "\n",
    "\n",
    "model_withoutSV = svm_train(Y_train_1,X_train_1, \"-s 0 -c 100 -t 0\")\n",
    "model_withoutNotSV = svm_train(Y_train_2,X_train_2, \"-s 0 -c 100 -t 0\")\n",
    "\n",
    "\n",
    "\n",
    "weights_1,sv_ind_1=(weightFinder(model_withoutSV))\n",
    "weights_2,sv_ind_2=weightFinder(model_withoutNotSV)\n",
    "\n",
    "print()\n",
    "print(\"Weights after removal of a SV data point:\")\n",
    "print(weights_1)\n",
    "print(\"SV indices after removal of a SV data point: \")\n",
    "print(sv_ind_1)\n",
    "print(\"Set difference between SV's indices before and after removal of a SV data point.\")\n",
    "print(np.setdiff1d(sv_ind_1,sv_ind))\n",
    "print()\n",
    "print(\"Weights after removal of a non-SV data point:\")\n",
    "print(weights_2)\n",
    "print(\"SV indices after removal of a non-SV data point: \")\n",
    "print(sv_ind_2)\n",
    "print(\"Set difference between SV's indices before and after removal of a non-SV data point.\")\n",
    "print(np.setdiff1d(sv_ind_2,sv_ind))\n",
    "\n",
    "#When we delete SV point, weights and support vector set changed.\n",
    "#When we delete non-SV point, weights changed very little and support vector set does not changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task - 10 pts\n",
    "\n",
    "Use Python and [CVXOPT](http://cvxopt.org) QP solver to implement the hard margin SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
